<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Chapter-7 Deep Learning for Coders with FastAI & Pytorch | Ravi Chandra Veeramachaneni</title><meta name=keywords content><meta name=description content="Training State-Of-The-Art-Model"><meta name=author content="Ravi Chandra Veeramachaneni"><link rel=canonical href=//r-c.ai/blogs/b6/ch7-dl-for-coders/><link crossorigin=anonymous href=/assets/css/stylesheet.min.3a8b00d8b9704de6f4f33d0a113c1892c930ae073602ee85b7c5937497c98078.css integrity="sha256-OosA2LlwTeb08z0KETwYkskwrgc2Au6Ft8WTdJfJgHg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=//r-c.ai/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=//r-c.ai/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=//r-c.ai/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=//r-c.ai/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=//r-c.ai/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-B5ZRHBFS10"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B5ZRHBFS10",{anonymize_ip:!1})}</script><meta property="og:title" content="Chapter-7 Deep Learning for Coders with FastAI & Pytorch"><meta property="og:description" content="Training State-Of-The-Art-Model"><meta property="og:type" content="article"><meta property="og:url" content="//r-c.ai/blogs/b6/ch7-dl-for-coders/"><meta property="og:image" content="//r-c.ai/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2021-08-04T00:00:00+00:00"><meta property="article:modified_time" content="2021-08-04T00:00:00+00:00"><meta property="og:site_name" content="RaviChandraVeeramachaneni"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="//r-c.ai/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Chapter-7 Deep Learning for Coders with FastAI & Pytorch"><meta name=twitter:description content="Training State-Of-The-Art-Model"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"//r-c.ai/blogs/"},{"@type":"ListItem","position":2,"name":"Chapter-7 Deep Learning for Coders with FastAI \u0026 Pytorch","item":"//r-c.ai/blogs/b6/ch7-dl-for-coders/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Chapter-7 Deep Learning for Coders with FastAI \u0026 Pytorch","name":"Chapter-7 Deep Learning for Coders with FastAI \u0026 Pytorch","description":"Training State-Of-The-Art-Model","keywords":[],"articleBody":"Keypoints Introduction  It is better to fail fast than very late. And it is always better to run more experiments on a smaller dataset rather running a single experiment on a large dataset.\n  This chapter introduces a new dataset called Imagenette1   Imagenette is a subset of the original Imagenet dataset but has only 10 categories of classes which are very different. This dataset has full-size, full-color images, which are photos of objects of different sizes, in different orientations, in different lighting, and so forth.  Imagenette  Important message: the dataset you get given is not necessarily the dataset you want.\n  This dataset has been created by fast.ai team to quickly experiment with the ideas and to give the opportunity to iterate quickly. Lets see how can we work with this dataset and then apply techniques which can be used on larger datasets like Imagenet as well.  Step 1: Downloading dataset \u0026 building The Datablock\n1 2 3 4 5 6 7  path = untar_data(URLs.IMAGENETTE) dblock = DataBlock(blocks=(ImageBlock(), CategoryBlock()),  get_items=get_image_files,  get_y=parent_label,  item_tfms=Resize(460),  batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = dblock.dataloaders(path, bs=64)   Step 2: Creating a Baseline \u0026 Training the model\n1 2 3  model = xresnet50(n_out=dls.c) learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(5, 3e-3)   All the results are from my colab2 So far we have achieved about 83.3% of accuracy. Let’s try to apply some techniques that would improve the performance.  Normalization  One of the strategy in the data pre-processing that will help a model perform better is to normalize the data.\n  Data which has mean of 0 and a standard deviation of 1 is referred as Normalized data. But most of the data like images used is in between 0 to 255 pixels or between 0 \u0026 1. So, we do not have the normalized data in either case. So to normalize the data, in fastAI we can pass Normalize transform. This transform will take the mean and standard deviation we want and transform the data accordingly. Normalization is an important technique that can be used when using pre-trained models.  Note: When using the cnn_learner with a pre-trained problem, we need not add the Normalize transform since the fastAI library automatically adds it.\nStep 3: Adding Normalization\n1 2 3 4 5 6 7  def get_dls(bs, size):  dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),  get_items=get_image_files,  get_y=parent_label,  item_tfms=Resize(460),  batch_tfms=[*aug_transforms(size=size, min_scale=0.75),Normalize.from_stats(*imagenet_stats)])  return dblock.dataloaders(path, bs=bs)   Step 4: Training the model again with normalization added\n After normalization, we have achieved an accuracy of 82.2%. Not an huge improvement from the previous, but for some strange reason there is a slight drop in performnace.   The other technique we can employ here for training is ti start small and then increase as required. All the above steps are confined to train images which are at size 224. So, we can start with much smaller size and increase it and this technique is called Progressive Resizing.  Progressive Resizing  Progressive resizing: Gradually using larger and larger images as you train.\n  Spending most of the epochs training with small images, helps training complete much faster. Completing training using large images makes the final accuracy much higher. In the process, since we will be using different size of images, we can use fine_tune to tune the model. And if we closely observed this is kind of a data augmentation technique.  Step 5: Create a data loader and try to fit into the model.\n1 2 3 4  dls = get_dls(128, 128) learn = Learner(dls, xresnet50(n_out=dls.c), loss_func=CrossEntropyLossFlat(),  metrics=accuracy) learn.fit_one_cycle(4, 3e-3)   Step 6: Replace the data loader and fine_tune it.\n1 2  learn.dls = get_dls(64, 224) learn.fine_tune(5, 1e-3)    From the above step it is evident that the Progressive resizing has achieved a good improvement in the accuracy of about 86.3%. However, it’s important to understand that the size of the Image at maximum could be the size of the image available on disk. Also, another caution on the resizing part is to not damage the pertained weights. This might happen if we have the the pre-trained weights similar to the weights in the transfer learning. The next technique to apply to the model is to apply data augmentation to the test set.  Test time Augmentation  Test Time Augmentation (TTA): During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.\n  Traditionally we use to perform training data augmentation with different techniques. When it comes to validation set, the fastAI for instance applies the center cropping. Center cropping is useful in some use cases but not all. This is because cropping from center may entirely discard any images on the borders. Instead on way would be to stretch and squish instead of cropping. However this becomes a hectic problem for model to learn those new patterns. Another way would be to select a number of areas to crop from the original rectangular image, pass each of them through our model, and take the maximum or average of the predictions. We could do this around different values across all of our test time augmentation parameters. This is known as /test time augmentation/ (TTA).  Step 7: Trying TTA\n1 2  preds,targs = learn.tta() accuracy(preds, targs).item()    We can see that the above technique has turned out well on improving accuracy to about 87.5%. However the above process slows down the inference by number of times we are averaging for TTA. So we can try another technique called Mixup.  Mixup  Mixup is a very powerful data augmentation technique that can provide dramatically higher accuracy, especially when you don’t have much data and don’t have a pre-trained model that was trained on data similar to your dataset. Mixup technique talks about the data augmentation for the specific kind of dataset and fine tuned as needed.  Mixup works as follows, for each image:\n Picking a random image from your dataset. Picking a weight at random. Taking a weighted average (from step 2) of the selected image with your image; this will be your independent variable. Taking a weighted average (with the same weight) of this image’s labels with your image’s labels; this will be your dependent variable.   Note: For mixup, the targets need to be one-hot encoded.\n  One of the reasons that Mixup is so exciting is that it can be applied to types of data other than photos. But, the issue with this technique might be the labels getting bigger than 0 or smaller than one as opposed to the one-hot encodings. So, we can handle this through label smoothing.  Label Smoothing  In “Classification Problems”, our targets are one-hot encoded, which means we have the the model return either 0 or 1. Even a smallest of the difference like 0.999 will encourage the model to overfit and at inference the model that is not going to give meaningful probabilities. Instead to avoid this we could replace all our 1s with a number a bit less than 1, and our 0s by a number a bit more than 0, and then train. This is called label smoothing. And this will make the model generalize better.  Conclusion  All the techniques described above are kind of eye opening on how we can build techniques that could augment each other and sometimes better than others. All these techniques will be applied to a real dataset and results will be published soon with description.  Appendix   Imagenette ↩︎\n All the results are from my colab ↩︎\n   ","wordCount":"1232","inLanguage":"en","datePublished":"2021-08-04T00:00:00Z","dateModified":"2021-08-04T00:00:00Z","author":{"@type":"Person","name":"Ravi Chandra Veeramachaneni"},"mainEntityOfPage":{"@type":"WebPage","@id":"//r-c.ai/blogs/b6/ch7-dl-for-coders/"},"publisher":{"@type":"Organization","name":"Ravi Chandra Veeramachaneni","logo":{"@type":"ImageObject","url":"//r-c.ai/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=//r-c.ai/ accesskey=h title="Ravi Chandra Veeramachaneni (Alt + H)"><img src=//r-c.ai/images/function.png alt=logo aria-label=logo height=35>Ravi Chandra Veeramachaneni</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=//r-c.ai/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=//r-c.ai/projects/ title=Projects><span>Projects</span></a></li><li><a href=//r-c.ai/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=//r-c.ai/>Home</a>&nbsp;»&nbsp;<a href=//r-c.ai/blogs/>Blogs</a></div><h1 class=post-title>Chapter-7 Deep Learning for Coders with FastAI & Pytorch</h1><div class=post-description>Training State-Of-The-Art-Model</div><div class=post-meta><span title="2021-08-04 00:00:00 +0000 UTC">August 4, 2021</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Ravi Chandra Veeramachaneni</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#keypoints aria-label=Keypoints>Keypoints</a><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#imagenette aria-label=Imagenette>Imagenette</a></li><li><a href=#normalization aria-label=Normalization>Normalization</a></li><li><a href=#progressive-resizing aria-label="Progressive Resizing">Progressive Resizing</a></li><li><a href=#test-time-augmentation aria-label="Test time Augmentation">Test time Augmentation</a></li><li><a href=#mixup aria-label=Mixup>Mixup</a></li><li><a href=#label-smoothing aria-label="Label Smoothing">Label Smoothing</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></li><li><a href=#appendix aria-label=Appendix>Appendix</a></li></ul></div></details></div><div class=post-content><h3 id=keypoints>Keypoints<a hidden class=anchor aria-hidden=true href=#keypoints>#</a></h3><h4 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h4><blockquote><p>It is better to fail fast than very late. And it is always better to run more experiments on a smaller dataset rather running a single experiment on a large dataset.</p></blockquote><ul><li>This chapter introduces a new dataset called Imagenette<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></li></ul><ul><li>Imagenette is a subset of the original Imagenet dataset but has only 10 categories of classes which are very different. This dataset has full-size, full-color images, which are photos of objects of different sizes, in different orientations, in different lighting, and so forth.</li></ul><h4 id=imagenette>Imagenette<a hidden class=anchor aria-hidden=true href=#imagenette>#</a></h4><blockquote><p>Important message: the dataset you get given is not necessarily the dataset you want.</p></blockquote><ul><li>This dataset has been created by fast.ai team to quickly experiment with the ideas and to give the opportunity to iterate quickly.</li><li>Lets see how can we work with this dataset and then apply techniques which can be used on larger datasets like Imagenet as well.</li></ul><p><strong>Step 1:</strong> Downloading dataset & building The Datablock</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>path <span style=color:#f92672>=</span> untar_data(URLs<span style=color:#f92672>.</span>IMAGENETTE)
</span></span><span style=display:flex><span>dblock <span style=color:#f92672>=</span> DataBlock(blocks<span style=color:#f92672>=</span>(ImageBlock(), CategoryBlock()),
</span></span><span style=display:flex><span>                   get_items<span style=color:#f92672>=</span>get_image_files,
</span></span><span style=display:flex><span>                   get_y<span style=color:#f92672>=</span>parent_label,
</span></span><span style=display:flex><span>                   item_tfms<span style=color:#f92672>=</span>Resize(<span style=color:#ae81ff>460</span>),
</span></span><span style=display:flex><span>                   batch_tfms<span style=color:#f92672>=</span>aug_transforms(size<span style=color:#f92672>=</span><span style=color:#ae81ff>224</span>, min_scale<span style=color:#f92672>=</span><span style=color:#ae81ff>0.75</span>))
</span></span><span style=display:flex><span>dls <span style=color:#f92672>=</span> dblock<span style=color:#f92672>.</span>dataloaders(path, bs<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>) 
</span></span></code></pre></td></tr></table></div></div><p><strong>Step 2:</strong> Creating a Baseline & Training the model</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#f92672>=</span> xresnet50(n_out<span style=color:#f92672>=</span>dls<span style=color:#f92672>.</span>c)
</span></span><span style=display:flex><span>learn <span style=color:#f92672>=</span> Learner(dls, model, loss_func<span style=color:#f92672>=</span>CrossEntropyLossFlat(), metrics<span style=color:#f92672>=</span>accuracy)
</span></span><span style=display:flex><span>learn<span style=color:#f92672>.</span>fit_one_cycle(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>3e-3</span>)
</span></span></code></pre></td></tr></table></div></div><p><img loading=lazy src=/blogs/b6/table1.png alt=table1>
All the results are from my colab<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><ul><li>So far we have achieved about 83.3% of accuracy. Let’s try to apply some techniques that would improve the performance.</li></ul><h4 id=normalization>Normalization<a hidden class=anchor aria-hidden=true href=#normalization>#</a></h4><blockquote><p>One of the strategy in the data pre-processing that will help a model perform better is to normalize the data.</p></blockquote><ul><li>Data which has mean of 0 and a standard deviation of 1 is referred as Normalized data. But most of the data like images used is in between 0 to 255 pixels or between 0 & 1.</li><li>So, we do not have the normalized data in either case. So to normalize the data, in fastAI we can pass Normalize transform.</li><li>This transform will take the mean and standard deviation we want and transform the data accordingly.</li><li><strong>Normalization</strong> is an important technique that can be used when using pre-trained models.</li></ul><p><strong>Note:</strong> When using the cnn_learner with a pre-trained problem, we need not add the Normalize transform since the fastAI library automatically adds it.</p><p><strong>Step 3:</strong> Adding Normalization</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_dls</span>(bs, size):
</span></span><span style=display:flex><span>    dblock <span style=color:#f92672>=</span> DataBlock(blocks<span style=color:#f92672>=</span>(ImageBlock, CategoryBlock),
</span></span><span style=display:flex><span>                   get_items<span style=color:#f92672>=</span>get_image_files,
</span></span><span style=display:flex><span>                   get_y<span style=color:#f92672>=</span>parent_label,
</span></span><span style=display:flex><span>                   item_tfms<span style=color:#f92672>=</span>Resize(<span style=color:#ae81ff>460</span>),
</span></span><span style=display:flex><span>                   batch_tfms<span style=color:#f92672>=</span>[<span style=color:#f92672>*</span>aug_transforms(size<span style=color:#f92672>=</span>size, min_scale<span style=color:#f92672>=</span><span style=color:#ae81ff>0.75</span>),Normalize<span style=color:#f92672>.</span>from_stats(<span style=color:#f92672>*</span>imagenet_stats)])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dblock<span style=color:#f92672>.</span>dataloaders(path, bs<span style=color:#f92672>=</span>bs)
</span></span></code></pre></td></tr></table></div></div><p><strong>Step 4:</strong> Training the model again with normalization added</p><ul><li>After normalization, we have achieved an accuracy of 82.2%. Not an huge improvement from the previous, but for some strange reason there is a slight drop in performnace.</li></ul><p><img loading=lazy src=/blogs/b6/table2.png alt=table2></p><ul><li>The other technique we can employ here for training is ti start small and then increase as required.</li><li>All the above steps are confined to train images which are at size 224. So, we can start with much smaller size and increase it and this technique is called <strong>Progressive Resizing</strong>.</li></ul><h4 id=progressive-resizing>Progressive Resizing<a hidden class=anchor aria-hidden=true href=#progressive-resizing>#</a></h4><blockquote><p>Progressive resizing: Gradually using larger and larger images as you train.</p></blockquote><ul><li>Spending most of the epochs training with small images, helps training complete much faster.</li><li>Completing training using large images makes the final accuracy much higher. In the process, since we will be using different size of images, we can use fine_tune to tune the model.</li><li>And if we closely observed this is kind of a data augmentation technique.</li></ul><p><strong>Step 5:</strong> Create a data loader and try to fit into the model.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dls <span style=color:#f92672>=</span> get_dls(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>)
</span></span><span style=display:flex><span>learn <span style=color:#f92672>=</span> Learner(dls, xresnet50(n_out<span style=color:#f92672>=</span>dls<span style=color:#f92672>.</span>c), loss_func<span style=color:#f92672>=</span>CrossEntropyLossFlat(), 
</span></span><span style=display:flex><span>                metrics<span style=color:#f92672>=</span>accuracy)
</span></span><span style=display:flex><span>learn<span style=color:#f92672>.</span>fit_one_cycle(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>3e-3</span>)
</span></span></code></pre></td></tr></table></div></div><p><img loading=lazy src=/blogs/b6/table3.png alt=table3></p><p><strong>Step 6:</strong> Replace the data loader and fine_tune it.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>learn<span style=color:#f92672>.</span>dls <span style=color:#f92672>=</span> get_dls(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>224</span>)
</span></span><span style=display:flex><span>learn<span style=color:#f92672>.</span>fine_tune(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>1e-3</span>)
</span></span></code></pre></td></tr></table></div></div><p><img loading=lazy src=/blogs/b6/table4.png alt=table4></p><ul><li>From the above step it is evident that the Progressive resizing has achieved a good improvement in the accuracy of about 86.3%.</li><li>However, it’s important to understand that the size of the Image at maximum could be the size of the image available on disk.</li><li>Also, another caution on the resizing part is to not damage the pertained weights.</li><li>This might happen if we have the the pre-trained weights similar to the weights in the transfer learning.</li><li>The next technique to apply to the model is to apply data augmentation to the test set.</li></ul><h4 id=test-time-augmentation>Test time Augmentation<a hidden class=anchor aria-hidden=true href=#test-time-augmentation>#</a></h4><blockquote><p>Test Time Augmentation (TTA): During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.</p></blockquote><ul><li>Traditionally we use to perform training data augmentation with different techniques.</li><li>When it comes to validation set, the fastAI for instance applies the center cropping.</li><li>Center cropping is useful in some use cases but not all.</li><li>This is because cropping from center may entirely discard any images on the borders. Instead on way would be to stretch and squish instead of cropping.</li><li>However this becomes a hectic problem for model to learn those new patterns.</li><li>Another way would be to select a number of areas to crop from the original rectangular image, pass each of them through our model, and take the maximum or average of the predictions.</li><li>We could do this around different values across all of our test time augmentation parameters. This is known as /test time augmentation/ (TTA).</li></ul><p><strong>Step 7:</strong> Trying TTA</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>preds,targs <span style=color:#f92672>=</span> learn<span style=color:#f92672>.</span>tta()
</span></span><span style=display:flex><span>accuracy(preds, targs)<span style=color:#f92672>.</span>item() 
</span></span></code></pre></td></tr></table></div></div><p><img loading=lazy src=/blogs/b6/table5.png alt=table5></p><ul><li>We can see that the above technique has turned out well on improving accuracy to about 87.5%.</li><li>However the above process slows down the inference by number of times we are averaging for TTA. So we can try another technique called <strong>Mixup</strong>.</li></ul><h4 id=mixup>Mixup<a hidden class=anchor aria-hidden=true href=#mixup>#</a></h4><ul><li>Mixup is a very powerful data augmentation technique that can provide dramatically higher accuracy, especially when you don’t have much data and don’t have a pre-trained model that was trained on data similar to your dataset.</li><li>Mixup technique talks about the data augmentation for the specific kind of dataset and fine tuned as needed.</li></ul><p><strong>Mixup works as follows, for each image:</strong></p><ul><li>Picking a random image from your dataset.</li><li>Picking a weight at random.</li><li>Taking a weighted average (from step 2) of the selected image with your image; this will be your independent variable.</li><li>Taking a weighted average (with the same weight) of this image’s labels with your image’s labels; this will be your dependent variable.</li></ul><blockquote><p>Note: For mixup, the targets need to be <strong>one-hot encoded</strong>.</p></blockquote><ul><li>One of the reasons that Mixup is so exciting is that it can be applied to types of data other than photos.</li><li>But, the issue with this technique might be the labels getting bigger than 0 or smaller than one as opposed to the one-hot encodings.</li><li>So, we can handle this through label smoothing.</li></ul><h4 id=label-smoothing>Label Smoothing<a hidden class=anchor aria-hidden=true href=#label-smoothing>#</a></h4><ul><li>In “Classification Problems”, our targets are one-hot encoded, which means we have the the model return either 0 or 1.</li><li>Even a smallest of the difference like 0.999 will encourage the model to overfit and at inference the model that is not going to give meaningful probabilities.</li><li>Instead to avoid this we could replace all our 1s with a number a bit less than 1, and our 0s by a number a bit more than 0, and then train. This is called <strong>label smoothing</strong>.</li><li>And this will make the model generalize better.</li></ul><h4 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h4><ul><li>All the techniques described above are kind of eye opening on how we can build techniques that could augment each other and sometimes better than others.</li><li>All these techniques will be applied to a real dataset and results will be published soon with description.</li></ul><h3 id=appendix>Appendix<a hidden class=anchor aria-hidden=true href=#appendix>#</a></h3><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://github.com/fastai/imagenette>Imagenette</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://gist.github.com/RaviChandraVeeramachaneni/e6b62ec22dc464d569d3b1ccf9f28d5c>All the results are from my colab</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><nav class=paginav><a class=prev href=//r-c.ai/blogs/b7/ch8-dl-for-coders/><span class=title>« Prev Page</span><br><span>Chapter-8 Deep Learning for Coders with FastAI & Pytorch</span></a>
<a class=next href=//r-c.ai/blogs/b5/ch6-dl-for-coders/><span class=title>Next Page »</span><br><span>Chapter-6 Deep Learning for Coders with FastAI & Pytorch</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=//r-c.ai/>Ravi Chandra Veeramachaneni</a></span>
<span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>