<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Chapter-4 Deep Learning for Coders with FastAI & Pytorch | Ravi Chandra Veeramachaneni</title><meta name=keywords content><meta name=description content="Training a Digit Classifier"><meta name=author content="Ravi Chandra Veeramachaneni"><link rel=canonical href=//r-c.ai/blogs/b3/ch4-dl-for-coders/><link crossorigin=anonymous href=/assets/css/stylesheet.min.3a8b00d8b9704de6f4f33d0a113c1892c930ae073602ee85b7c5937497c98078.css integrity="sha256-OosA2LlwTeb08z0KETwYkskwrgc2Au6Ft8WTdJfJgHg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=//r-c.ai/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=//r-c.ai/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=//r-c.ai/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=//r-c.ai/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=//r-c.ai/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-B5ZRHBFS10"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B5ZRHBFS10",{anonymize_ip:!1})}</script><meta property="og:title" content="Chapter-4 Deep Learning for Coders with FastAI & Pytorch"><meta property="og:description" content="Training a Digit Classifier"><meta property="og:type" content="article"><meta property="og:url" content="//r-c.ai/blogs/b3/ch4-dl-for-coders/"><meta property="og:image" content="//r-c.ai/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2021-06-30T00:00:00+00:00"><meta property="article:modified_time" content="2021-06-30T00:00:00+00:00"><meta property="og:site_name" content="RaviChandraVeeramachaneni"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="//r-c.ai/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Chapter-4 Deep Learning for Coders with FastAI & Pytorch"><meta name=twitter:description content="Training a Digit Classifier"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"//r-c.ai/blogs/"},{"@type":"ListItem","position":2,"name":"Chapter-4 Deep Learning for Coders with FastAI \u0026 Pytorch","item":"//r-c.ai/blogs/b3/ch4-dl-for-coders/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Chapter-4 Deep Learning for Coders with FastAI \u0026 Pytorch","name":"Chapter-4 Deep Learning for Coders with FastAI \u0026 Pytorch","description":"Training a Digit Classifier","keywords":[],"articleBody":"Keypoints Lets recognize the Hand Written Digits via MNIST database.  A image is an array of numbers / pixels under the hood. We need this because a computer only understand numbers. So, we try to convert all images into numbers and hold them in a data structure like Numpy Array or PyTorch tensor which will make the computations easier. Each Image is two dimensional in nature since they are grey scaled. All the images are now stacked to create a single tensor of 3 dimensions which will help to calculate the mean of each pixel. Now we do this mean calculation to achieve a single image of 3 or 7, which our model learns as an ideal 3 or 7. Now given an image of 3 or 7 whether from a training set or validation set we can use distance techniques like L1 norm or L2 norm to compute the distance between that image and the ideal 3 or 7 image. So the accuracy is the metric we will consider for knowing how good our model is performing.  Simple Example:\n1 2 3 4 5 6 7  # Using Numpy Arrays: array(img) # To get full array  array(img)[i:j, i:j] # To get the partial array we can provide rows, cols  # Using PyTorch tensors: tensor(img) # To get full tensor tensor(img3)[i:j, i:j] # To get the partial tensor we can provide rows, cos    Both Numpy Array \u0026 PyTorch Tensor are same except for the naming. An Numpy array is a simple 2-dimensional representation of the image where as the PyTorch tensor is a multi-dimensional representation of the image. The below image shows the example of an image illustrated as an array \u0026 tensor and its dimensions.  Note: Since the image is a simple grey image (No Color), we have just 2 dimensions. If it’s an color image than we would have 3 dimensions (R,G,B).\n Each number in the image array is called a Pixel \u0026 each pixel is in between 0 to 255. The MNIST images are 28 * 28 (total image size 784 pixels). A Baseline is a simple model which will perform reasonably well. So it is always better to start with a baseline and keep improving the model to see how the new ideas improve the model performance/accuracy. Rank of a Tensor: A Rank of a tensor is simply the number of dimensions or axes. For instance, a three dimensional tensor can also be referred as rank-3 tensor. Shape of Tensor: Shape is the size of each axis of the tensor. To get an ideal image of 3 or 7, we need to perform the below steps: (Each Function shown here is explained in detailed with examples in the next section). Convert all the images into tensors using tensor(Image.open(each_image)). Wrap the converted images into a list of image tensor’s. Stack all the list of image tensors so that it creates a single tensor of rank-3 using torch.stack(list_of_image_tensors). In simple terms a 3-dimensional tensor. If needed we have to cast all of the values to float for calculating mean by using float() function from pytorch library. Final step is to take the mean of the image tensors along dimension-0 for the above stacked rank-3 tensor using mean() function. For every pixel position, the mean() will compute the average of that pixel over all the images. The result, is the tensor of an ideal 3 or 7 calculated by computing the mean of all the images which will have 2-dimensions like our original images. To classify if an Image is a 3 or 7, we can use a simple technique like finding the distance of that image from an ideal 3 or ideal 7 computed in the earlier steps. Distance calculation can be done using either L1 normalization or L2 normalization. A simple distance measure like adding up differences between the pixels would not yield accurate results due to positive \u0026 negative values (Note: Why Negative Values ? Remember we have lot of 0’s in the image and other image may contain a value at that same pixel which would result in a negative value). L1 norm: Taking the mean of the absolute value of the differences between 2 image tensors. A absolute value abs() is a function that replaces negative values with positive values. This is also referred as Mean Absolute difference. This can be calculated using F.l1_loss(any_3.float(), ideal_3). L2 norm: Taking the mean of the square of the differences and then take a square root. So Squaring a difference will make the value positive \u0026 then square root cancels the square effect. This is also referred to as Root Mean Squared Error (RMSE). This can be calculated using F.mse_loss(any_3.float(), ideal_3). The result of the above computation would yield the loss. Higher the loss, lesser the confidence of that image being 3 vs 7. In practice, we use accuracy as the metric for our classification models. It is computed over the training data to make sure overfitting occurs. Broadcasting is a technique which will automatically expand the tensor with the smaller rank to have same sizes as one with the larger rank.  1 2 3  tensor([1,2,3]) + tensor(1)  Output: tensor([2, 3, 4])    In broadcasting technique the PyTorch never creates copies of lower ranked tensor.  Some of the new things learned from fastAI library:  ls() → L - A function that list the count of items \u0026 contents of the directory. It returns a fastai class called L which is similar to python built-in List class with additional features. Image.open() → PngImageFile: A class from Python Image Library (PIL) used for operations on images like viewing, opening \u0026 manipulation.  1  image_to_display = Image.open(path_to_image)    A Pandas.DataFrame(image) → DataFrame: A function that takes an image, converts that into a DataFrame Object \u0026 returns it. We can set some style properties on a dataframe to see the color coding and understand it better.  1 2  df = pd.DataFrame(image[i:j,i:j]) df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')    np.array(img) → Numpy.Array: Numpy’s array function will take an image and return the pixels of the image in a two dimensional data structure called Array. tensor(img) → PyTorch.Tensor: PyTorch’s tensor function will take an image and return the pixels of the image in a multi-dimensional data structure called Tensor. List Comprehensions in python returns each value in the given list based on a optional condition when passed to any function f().  1 2 3 4 5 6 7 8 9  new_list = [f(o) for o in a_list if o0]  - a_list: The list we want to perfrom the opertaions.  - o  0: A optional condition which every element will obey in this case greater than zero.  - f(o): Something to do with each element  # Example: seven_tensors = [tensor(Image.open(o)) for o in sevens]  - sevens: Our list of images of sevens  - tensor(Image.open(image)): A function that needs to be executed on each image, in this case opening the image \u0026 creating it as atensor.    show_image(tensor) → Image: A fastai function which takes an image tensor \u0026 display the image. torch.stack(list_of_tensors) → PyTorch.Tensor: A Pytorch function that takes a list of tensors (2-dim image tensors) and create a 3-dimensional single tensor (rank-3 tensor). This will be useful to compute the average across all the images.    torch.tensor.float() → float_value: Casting the values from int to float in PyTorch gives the ability to calculate the mean.\n L1 norm can be performed using the following  1 2 3  dist_3_abs = (any_image_of_3 - ideal_3).abs().mean()  # Note: ideal_3 is the 3 calculated by compuitng mean of the stacked rank-3 tensor.    L2 norm can be performed using the following  1  dist_3_sqr = ((any_image_of_3 - ideal_3)**2).mean().sqrt()    The above distances can be also computed using the inbuilt PyTorch lib functions from torch.nn.functional package which is by default imported as F by fastai as recommended.  1 2 3 4 5  # L1 norm: F.l1_loss(any_3.float(),mean7)  # L2 norm: F.mse_loss(any_3,mean7).sqrt()     Stochastic Gradient Descent is the way of automatically updating the weights of the neural network input’s, based on the previous result to gain the maximum performance or in simple terms better accuracy.\n  This can be made entirely automated, so that the network can reach back to the initial inputs, update their weights and can perform the training again with the new weights. This process is called the back-propagation.\n  Example of an function that can be used to classify a number based on the above described way:\n  1 2  def pr_eight(x,w):  return (x*w).sum()    In the above function, x - vector representation of the input Image. w - vector of weights  How can we make this function into a Machine Learning Classifier:\n Initialize the weights. For each image use these weights to predict whether a 3 or a 7. Calculate the loss for this model based on these predictions. Calculate the gradient, which helps to determine the change in the weight and in turn the loss for that weight. And this has to be done for each weight. Change the weights based on the above gradient calculation. This step is called “Step”. Now we need to repeat from prediction step (step 2). Iterate until your model is good enough.  Detailing the each step in the above process:\n Initialize: Initializing the parameters/ Weights to random values will perfectly work. Loss: We need a function that return the loss in terms of a number. A good model has small loss and vice versa. Step: We need to determine whether to increase the weights or decrease the weights to maximize the performance or in other terms minimize loss. Once we determine the increase or decrease then we can increment/decrement accordingly in small amounts and check at which point we are achieving the maximum performance. This process is manual and slower and can be automated and achieved by calculating gradient using calculus. Gradient calculation will figure out directly whether to increment / decrement weights and by how much amount. Stop: This is where we will decide \u0026 implement about number of epochs to train our model . In the case of digit classifier, we will train our model until over fitting (Our model performance gets worse) occurs.  Example of a simple loss function and understand about slope\n1 2 3 4 5 6  def f(x): ''' Simple quadratic loss function x: weight parameter '''\t\treturn x**2    Visualizing the above function with slope at one point , when initialized it with a random weight parameter.   Once we determine the direction of the slope, then we can keep adjusting the weight, calculate the loss every time and repeat the process until we reach the lowest point on the curve where the loss is minimum.   For gradient calculation, reason behind using calculus over doing it manually is to achieve performance optimization.  Calculating Gradients, Derivatives \u0026 why do we need them: What \u0026 Why:\n In simple words, gradient will tell us how much each weight has to be changed to make our model better. And it is a vector and it points in the direction of steepest ascent to minimize the loss. A derivate of a function is a number which tells us, how much a change in parameter will change the result of the function. For any quadratic function, we can calculate its derivative. Important Note: A derivative is also a function, which calculates the change, rather than a value like a normal function does. So, we need to calculate gradient to know how the function will change with a given value so that we can try to reduce the function to the smallest number where the loss is minimum. And the computational shortcut provided by calculus to do the gradient calculation is called Derivative.  How to we calculate derivates:\n We need to calculate gradient for every weight since we don’t have just one weight. We will calculate the derivative of one weight considering the others as constants and then repeat the process for every other weight. A pytorch example to calculate derivative at value 3  1  xt = tensor(3.).requires_grad_()    In deep learning, the “gradients” usually means the value of a function’s derivative at a particular argument value.  Example of a gradient calculation: Consider we want to calculate derivative of x*2 and the result is 2x, where x=3, so the gradient must be 2 x 3 = 6\n The gradient only tell us the slope of the functions and not exactly how much weight we have to adjust. But the intuition is if we have a big slope then we need to make lot of adjustments to weights and vice versa if the slope is small then we are almost close to optimal value.  How do we change Weights/Parameters based on Gradient Value:\n The most important part of the deep learning process is to decide how to change the parameters based on the gradient value. Simplest approach is to multiply the gradient with a small number often between 0.001 and 0.1 (but not limited to this range) and this is called Learning Rate(LR). Once we have a Learning Rate we can adjust our parameters using the below function This process is called as stepping the parameters using the Optimizer step. This is because, in this step we are trying to find an optimal weight. We can pick either very low learning rate or a very high learning rate and both have their consequences. If we have a low learning rate then we have to do lot of steps to get the optimal weight.   Picking a very high learning rate is even worse and can result in the loss getting worse(left image) or may bounce around(right image, requiring lot of steps to settle down) . So we are loosing our goal of minimizing the loss.  Conclusion:  We are trying to find the minimum (loss) using the SGD and this minimum can be used to train a model to fit the data better for our task.  Some miscellaneous Key-points:  All Machine learning datasets follow a common layout having separate folders for training and validation (test) set’s. A Numpy array \u0026 a PyTorch tensor are both multi-dimensional arrays and have similar capabilities except that the Numpy doesn’t have GPU support where as PyTorch does. PyTorch can automatically calculate derivates where as Numpy will not which is a very useful feature in terms of deeplearning.  ","wordCount":"2368","inLanguage":"en","datePublished":"2021-06-30T00:00:00Z","dateModified":"2021-06-30T00:00:00Z","author":{"@type":"Person","name":"Ravi Chandra Veeramachaneni"},"mainEntityOfPage":{"@type":"WebPage","@id":"//r-c.ai/blogs/b3/ch4-dl-for-coders/"},"publisher":{"@type":"Organization","name":"Ravi Chandra Veeramachaneni","logo":{"@type":"ImageObject","url":"//r-c.ai/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=//r-c.ai/ accesskey=h title="Ravi Chandra Veeramachaneni (Alt + H)"><img src=//r-c.ai/images/function.png alt=logo aria-label=logo height=35>Ravi Chandra Veeramachaneni</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=//r-c.ai/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=//r-c.ai/projects/ title=Projects><span>Projects</span></a></li><li><a href=//r-c.ai/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=//r-c.ai/>Home</a>&nbsp;»&nbsp;<a href=//r-c.ai/blogs/>Blogs</a></div><h1 class=post-title>Chapter-4 Deep Learning for Coders with FastAI & Pytorch</h1><div class=post-description>Training a Digit Classifier</div><div class=post-meta><span title="2021-06-30 00:00:00 +0000 UTC">June 30, 2021</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;Ravi Chandra Veeramachaneni</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#keypoints aria-label=Keypoints>Keypoints</a><ul><li><a href=#lets-recognize-the-hand-written-digits-via-mnist-database aria-label="Lets recognize the Hand Written Digits via MNIST database.">Lets recognize the Hand Written Digits via MNIST database.</a></li><li><a href=#some-of-the-new-things-learned-from-fastai-library aria-label="Some of the new things learned from fastAI library:">Some of the new things learned from fastAI library:</a></li><li><a href=#conclusion aria-label=Conclusion:>Conclusion:</a></li></ul></li><li><a href=#some-miscellaneous-key-points aria-label="Some miscellaneous Key-points:">Some miscellaneous Key-points:</a></li></ul></div></details></div><div class=post-content><h3 id=keypoints>Keypoints<a hidden class=anchor aria-hidden=true href=#keypoints>#</a></h3><h4 id=lets-recognize-the-hand-written-digits-via-mnist-database>Lets recognize the Hand Written Digits via MNIST database.<a hidden class=anchor aria-hidden=true href=#lets-recognize-the-hand-written-digits-via-mnist-database>#</a></h4><ul><li>A image is an array of numbers / pixels under the hood.</li><li>We need this because a computer only understand numbers.</li><li>So, we try to convert all images into numbers and hold them in a data structure like Numpy Array or PyTorch tensor which will make the computations easier.</li><li>Each Image is two dimensional in nature since they are grey scaled.</li><li>All the images are now stacked to create a single tensor of 3 dimensions which will help to calculate the mean of each pixel.</li><li>Now we do this mean calculation to achieve a single image of 3 or 7, which our model learns as an ideal 3 or 7.</li><li>Now given an image of 3 or 7 whether from a training set or validation set we can use distance techniques like L1 norm or L2 norm to compute the distance between that image and the ideal 3 or 7 image.</li><li>So the accuracy is the metric we will consider for knowing how good our model is performing.</li></ul><p>Simple Example:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Using Numpy Arrays:</span>
</span></span><span style=display:flex><span>array(img) <span style=color:#75715e># To get full array </span>
</span></span><span style=display:flex><span>array(img)[i:j, i:j] <span style=color:#75715e># To get the partial array we can provide rows, cols</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Using PyTorch tensors:</span>
</span></span><span style=display:flex><span>tensor(img) <span style=color:#75715e># To get full tensor</span>
</span></span><span style=display:flex><span>tensor(img3)[i:j, i:j] <span style=color:#75715e># To get the partial tensor we can provide rows, cos</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>Both Numpy Array & PyTorch Tensor are same except for the naming. An Numpy array is a simple 2-dimensional representation of the image where as the PyTorch tensor is a multi-dimensional representation of the image. The below image shows the example of an image illustrated as an array & tensor and its dimensions.</li></ul><p><strong>Note:</strong> Since the image is a simple grey image (No Color), we have just 2 dimensions. If it’s an color image than we would have 3 dimensions (R,G,B).</p><p><img loading=lazy src=/blogs/b3/rgb.png alt=rgb></p><ul><li>Each number in the image array is called a Pixel & each pixel is in between 0 to 255.</li><li>The MNIST images are 28 * 28 (total image size 784 pixels).</li><li>A <strong>Baseline</strong> is a simple model which will perform reasonably well. So it is always better to start with a baseline and keep improving the model to see how the new ideas improve the model performance/accuracy.</li><li><strong>Rank of a Tensor:</strong> A Rank of a tensor is simply the number of dimensions or axes. For instance, a three dimensional tensor can also be referred as rank-3 tensor.</li><li><strong>Shape of Tensor:</strong> Shape is the size of each axis of the tensor.</li><li>To get an ideal image of 3 or 7, we need to perform the below steps: (Each Function shown here is explained in detailed with examples in the next section).</li><li>Convert all the images into tensors using <code>tensor(Image.open(each_image))</code>.</li><li>Wrap the converted images into a list of image tensor’s.</li><li>Stack all the list of image tensors so that it creates a single tensor of rank-3 using torch.stack(list_of_image_tensors). In simple terms a 3-dimensional tensor.</li><li>If needed we have to cast all of the values to float for calculating mean by using float() function from pytorch library.</li><li>Final step is to take the mean of the image tensors along dimension-0 for the above stacked rank-3 tensor using mean() function. For every pixel position, the mean() will compute the average of that pixel over all the images.</li><li>The result, is the tensor of an ideal 3 or 7 calculated by computing the mean of all the images which will have 2-dimensions like our original images.</li><li>To classify if an Image is a 3 or 7, we can use a simple technique like finding the distance of that image from an ideal 3 or ideal 7 computed in the earlier steps.</li><li>Distance calculation can be done using either L1 normalization or L2 normalization. A simple distance measure like adding up differences between the pixels would not yield accurate results due to positive & negative values (Note: Why Negative Values ? Remember we have lot of 0’s in the image and other image may contain a value at that same pixel which would result in a negative value).</li><li><strong>L1 norm:</strong> Taking the mean of the absolute value of the differences between 2 image tensors. A absolute value abs() is a function that replaces negative values with positive values. This is also referred as Mean Absolute difference. This can be calculated using <code>F.l1_loss(any_3.float(), ideal_3)</code>.</li><li><strong>L2 norm:</strong> Taking the mean of the square of the differences and then take a square root. So Squaring a difference will make the value positive & then square root cancels the square effect. This is also referred to as Root Mean Squared Error (RMSE). This can be calculated using <code>F.mse_loss(any_3.float(), ideal_3)</code>.</li><li>The result of the above computation would yield the loss. Higher the loss, lesser the confidence of that image being 3 vs 7.</li><li>In practice, we use accuracy as the metric for our classification models. It is computed over the training data to make sure overfitting occurs.</li><li><strong>Broadcasting</strong> is a technique which will automatically expand the tensor with the smaller rank to have same sizes as one with the larger rank.</li></ul><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tensor([<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>]) <span style=color:#f92672>+</span> tensor(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Output: tensor([<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>])
</span></span></code></pre></td></tr></table></div></div><ul><li>In broadcasting technique the PyTorch never creates copies of lower ranked tensor.</li></ul><h4 id=some-of-the-new-things-learned-from-fastai-library>Some of the new things learned from fastAI library:<a hidden class=anchor aria-hidden=true href=#some-of-the-new-things-learned-from-fastai-library>#</a></h4><ul><li><strong>ls() → L</strong> - A function that list the count of items & contents of the directory. It returns a fastai class called L which is similar to python built-in List class with additional features.</li><li><strong>Image.open()</strong> → PngImageFile: A class from Python Image Library (PIL) used for operations on images like viewing, opening & manipulation.</li></ul><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>image_to_display <span style=color:#f92672>=</span> Image<span style=color:#f92672>.</span>open(path_to_image)
</span></span></code></pre></td></tr></table></div></div><ul><li>A <strong>Pandas.DataFrame(image)</strong> → DataFrame: A function that takes an image, converts that into a DataFrame Object & returns it.</li><li>We can set some style properties on a dataframe to see the color coding and understand it better.</li></ul><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(image[i:j,i:j])
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>style<span style=color:#f92672>.</span>set_properties(<span style=color:#f92672>**</span>{<span style=color:#e6db74>&#39;font-size&#39;</span>:<span style=color:#e6db74>&#39;6pt&#39;</span>})<span style=color:#f92672>.</span>background_gradient(<span style=color:#e6db74>&#39;Greys&#39;</span>)
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>np.array(img)</strong> → Numpy.Array: Numpy’s array function will take an image and return the pixels of the image in a two dimensional data structure called Array.</li><li><strong>tensor(img)</strong> → PyTorch.Tensor: PyTorch’s tensor function will take an image and return the pixels of the image in a multi-dimensional data structure called Tensor.</li><li>List Comprehensions in python returns each value in the given list based on a optional condition when passed to any function f().</li></ul><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">8
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">9
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>new_list <span style=color:#f92672>=</span> [f(o) <span style=color:#66d9ef>for</span> o <span style=color:#f92672>in</span> a_list <span style=color:#66d9ef>if</span> o<span style=color:#f92672>&gt;</span><span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span> <span style=color:#f92672>-</span> a_list: The list we want to perfrom the opertaions<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span> <span style=color:#f92672>-</span> o <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>: A optional condition which every element will obey <span style=color:#f92672>in</span> this case greater than zero<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span> <span style=color:#f92672>-</span> f(o): Something to do <span style=color:#66d9ef>with</span> each element
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Example:</span>
</span></span><span style=display:flex><span>seven_tensors <span style=color:#f92672>=</span> [tensor(Image<span style=color:#f92672>.</span>open(o)) <span style=color:#66d9ef>for</span> o <span style=color:#f92672>in</span> sevens]
</span></span><span style=display:flex><span> <span style=color:#f92672>-</span> sevens: Our list of images of sevens
</span></span><span style=display:flex><span> <span style=color:#f92672>-</span> tensor(Image<span style=color:#f92672>.</span>open(image)): A function that needs to be executed on each image, <span style=color:#f92672>in</span> this case opening the image <span style=color:#f92672>&amp;</span> creating it <span style=color:#66d9ef>as</span> atensor<span style=color:#f92672>.</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>show_image(tensor)</strong> → Image: A fastai function which takes an image tensor & display the image.</li><li><strong>torch.stack(list_of_tensors)</strong> → PyTorch.Tensor: A Pytorch function that takes a list of tensors (2-dim image tensors) and create a 3-dimensional single tensor (rank-3 tensor). This will be useful to compute the average across all the images.</li></ul><p><img loading=lazy src=/blogs/b3/tensor.png alt=tensor></p><ul><li><p><strong>torch.tensor.float()</strong> → float_value: Casting the values from int to float in PyTorch gives the ability to calculate the mean.</p><ul><li><strong>L1 norm</strong> can be performed using the following</li></ul><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist_3_abs <span style=color:#f92672>=</span> (any_image_of_3 <span style=color:#f92672>-</span> ideal_3)<span style=color:#f92672>.</span>abs()<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Note: ideal_3 is the 3 calculated by compuitng mean of the stacked rank-3 tensor.</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>L2 norm</strong> can be performed using the following</li></ul><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist_3_sqr <span style=color:#f92672>=</span> ((any_image_of_3 <span style=color:#f92672>-</span> ideal_3)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>sqrt()
</span></span></code></pre></td></tr></table></div></div><ul><li>The above distances can be also computed using the inbuilt PyTorch lib functions from torch.nn.functional package which is by default imported as F by fastai as recommended.</li></ul><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># L1 norm:</span>
</span></span><span style=display:flex><span>F<span style=color:#f92672>.</span>l1_loss(any_3<span style=color:#f92672>.</span>float(),mean7)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># L2 norm:</span>
</span></span><span style=display:flex><span>F<span style=color:#f92672>.</span>mse_loss(any_3,mean7)<span style=color:#f92672>.</span>sqrt()
</span></span></code></pre></td></tr></table></div></div></li><li><p><strong>Stochastic Gradient Descent</strong> is the way of automatically updating the weights of the neural network input’s, based on the previous result to gain the maximum performance or in simple terms better accuracy.</p></li><li><p>This can be made entirely automated, so that the network can reach back to the initial inputs, update their weights and can perform the training again with the new weights. This process is called the back-propagation.</p></li><li><p>Example of an function that can be used to classify a number based on the above described way:</p></li></ul><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>pr_eight</span>(x,w):
</span></span><span style=display:flex><span> <span style=color:#66d9ef>return</span> (x<span style=color:#f92672>*</span>w)<span style=color:#f92672>.</span>sum()
</span></span></code></pre></td></tr></table></div></div><ul><li>In the above function, x - vector representation of the input Image. w - vector of weights</li></ul><p><strong>How can we make this function into a Machine Learning Classifier:</strong></p><ul><li>Initialize the weights.</li><li>For each image use these weights to predict whether a 3 or a 7.</li><li>Calculate the loss for this model based on these predictions.</li><li>Calculate the gradient, which helps to determine the change in the weight and in turn the loss for that weight. And this has to be done for each weight.</li><li>Change the weights based on the above gradient calculation. This step is called “Step”.</li><li>Now we need to repeat from prediction step (step 2).
Iterate until your model is good enough.</li></ul><p><img loading=lazy src=/blogs/b3/mlprog.png alt=mlporg></p><p><strong>Detailing the each step in the above process:</strong></p><ul><li><strong>Initialize:</strong> Initializing the parameters/ Weights to random values will perfectly work.</li><li><strong>Loss:</strong> We need a function that return the loss in terms of a number. A good model has small loss and vice versa.</li><li><strong>Step:</strong> We need to determine whether to increase the weights or decrease the weights to maximize the performance or in other terms minimize loss. Once we determine the increase or decrease then we can increment/decrement accordingly in small amounts and check at which point we are achieving the maximum performance. This process is manual and slower and can be automated and achieved by calculating gradient using calculus. Gradient calculation will figure out directly whether to increment / decrement weights and by how much amount.</li><li><strong>Stop:</strong> This is where we will decide & implement about number of epochs to train our model . In the case of digit classifier, we will train our model until over fitting (Our model performance gets worse) occurs.</li></ul><p><strong>Example of a simple loss function and understand about slope</strong></p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>f</span>(x):
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>	Simple quadratic loss function
</span></span></span><span style=display:flex><span><span style=color:#e6db74>	x: weight parameter
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>	
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> x<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>Visualizing the above function with slope at one point , when initialized it with a random weight parameter.</li></ul><p><img loading=lazy src=/blogs/b3/loss.png alt=loss></p><ul><li>Once we determine the direction of the slope, then we can keep adjusting the weight, calculate the loss every time and repeat the process until we reach the lowest point on the curve where the loss is minimum.</li></ul><p><img loading=lazy src=/blogs/b3/loss1.png alt=loss1></p><ul><li>For gradient calculation, reason behind using calculus over doing it manually is to achieve performance optimization.</li></ul><p><strong>Calculating Gradients, Derivatives & why do we need them: What & Why:</strong></p><ul><li>In simple words, gradient will tell us how much each weight has to be changed to make our model better. And it is a vector and it points in the direction of steepest ascent to minimize the loss.</li><li>A derivate of a function is a number which tells us, how much a change in parameter will change the result of the function.</li><li>For any quadratic function, we can calculate its derivative.</li><li>Important Note: A derivative is also a function, which calculates the change, rather than a value like a normal function does.</li><li>So, we need to calculate gradient to know how the function will change with a given value so that we can try to reduce the function to the smallest number where the loss is minimum.</li><li>And the computational shortcut provided by calculus to do the gradient calculation is called Derivative.</li></ul><p><strong>How to we calculate derivates:</strong></p><ul><li>We need to calculate gradient for every weight since we don’t have just one weight.</li><li>We will calculate the derivative of one weight considering the others as constants and then repeat the process for every other weight.</li><li>A pytorch example to calculate derivative at value 3</li></ul><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>xt <span style=color:#f92672>=</span> tensor(<span style=color:#ae81ff>3.</span>)<span style=color:#f92672>.</span>requires_grad_()
</span></span></code></pre></td></tr></table></div></div><ul><li>In deep learning, the “gradients” usually means the value of a function’s derivative at a particular argument value.</li></ul><p><strong>Example of a gradient calculation:</strong> Consider we want to calculate derivative of x*2 and the result is 2x, where x=3, so the gradient must be 2 x 3 = 6</p><p><img loading=lazy src=/blogs/b3/grad.png alt=grad></p><ul><li>The gradient only tell us the slope of the functions and not exactly how much weight we have to adjust. But the intuition is if we have a big slope then we need to make lot of adjustments to weights and vice versa if the slope is small then we are almost close to optimal value.</li></ul><p><strong>How do we change Weights/Parameters based on Gradient Value:</strong></p><ul><li>The most important part of the deep learning process is to decide how to change the parameters based on the gradient value.</li><li>Simplest approach is to multiply the gradient with a small number often between 0.001 and 0.1 (but not limited to this range) and this is called <strong>Learning Rate(LR)</strong>.</li><li>Once we have a Learning Rate we can adjust our parameters using the below function</li><li>This process is called as stepping the parameters using the Optimizer step. This is because, in this step we are trying to find an optimal weight.</li><li>We can pick either very low learning rate or a very high learning rate and both have their consequences.</li><li>If we have a low learning rate then we have to do lot of steps to get the optimal weight.</li></ul><p><img loading=lazy src=/blogs/b3/lr.png alt=lr></p><ul><li>Picking a very high learning rate is even worse and can result in the loss getting worse(left image) or may bounce around(right image, requiring lot of steps to settle down) . So we are loosing our goal of minimizing the loss.</li></ul><p><img loading=lazy src=/blogs/b3/min-loss.png alt=min-loss></p><h4 id=conclusion>Conclusion:<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h4><ul><li>We are trying to find the minimum (loss) using the SGD and this minimum can be used to train a model to fit the data better for our task.</li></ul><h3 id=some-miscellaneous-key-points>Some miscellaneous Key-points:<a hidden class=anchor aria-hidden=true href=#some-miscellaneous-key-points>#</a></h3><ul><li>All Machine learning datasets follow a common layout having separate folders for training and validation (test) set’s.</li><li>A Numpy array & a PyTorch tensor are both multi-dimensional arrays and have similar capabilities except that the Numpy doesn’t have GPU support where as PyTorch does.</li><li>PyTorch can automatically calculate derivates where as Numpy will not which is a very useful feature in terms of deeplearning.</li></ul></div><footer class=post-footer><nav class=paginav><a class=prev href=//r-c.ai/blogs/b4/ch5-dl-for-coders/><span class=title>« Prev Page</span><br><span>Chapter-5 Deep Learning for Coders with FastAI & Pytorch</span></a>
<a class=next href=//r-c.ai/blogs/b2/ch2-dl-for-coders/><span class=title>Next Page »</span><br><span>Chapter-2 Deep Learning for Coders with FastAI & Pytorch</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=//r-c.ai/>Ravi Chandra Veeramachaneni</a></span>
<span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>